"""
This file should be run after 7_eval_finetune.py. 
It loads the model checkpoints generated by that script, and evaluates them on the entire STARR test set.

Usage:
    python3 ../7b_eval_zero_shot.py \
    	--path_to_database '../../EHRSHOT_ASSETS/femr/extract' \
        --path_to_labels_dir '/share/pi/nigam/users/migufuen/ehrshot-benchmark/EHRSHOT_ASSETS/benchmark_ehrshot' \
        --path_to_features_dir '/share/pi/nigam/users/migufuen/ehrshot-benchmark/EHRSHOT_ASSETS/features_ehrshot' \
        --path_to_split_csv '/share/pi/nigam/users/migufuen/ehrshot-benchmark/EHRSHOT_ASSETS/splits_ehrshot/person_id_map.csv' \
        --path_to_tokenized_timelines_dir '/share/pi/nigam/users/migufuen/ehrshot-benchmark/EHRSHOT_ASSETS/tokenized_timelines_ehrshot' \
        --path_to_output_dir '../../EHRSHOT_ASSETS/results_ehrshot_zeroshot' \
        --shot_strat all \
        --labeling_function new_acutemi \
        --batch_size 64 \
        --models mamba-tiny-1024-att--clmbr_train-tokens-total_nonPAD-ckpt_val=2000000000-persist_chunk:last_embed:last
"""
import argparse
import json
import os
import pickle
from typing import Any, Dict, List, Optional, Tuple, Union, Callable, Set
import numpy as np
import collections
import pandas as pd
from tqdm import tqdm
import sklearn
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from torch.optim.lr_scheduler import LambdaLR
from loguru import logger
from sklearn.preprocessing import MaxAbsScaler
from femr.datasets import PatientDatabase
from ehrshot.utils import (
    LABELING_FUNCTION_2_PAPER_NAME,
    SHOT_STRATS,
    MODEL_2_INFO,
    get_labels_and_features, 
    process_chexpert_labels, 
    convert_multiclass_to_binary_labels,
    CHEXPERT_LABELS, 
    LR_PARAMS, 
    XGB_PARAMS, 
    RF_PARAMS,
    ProtoNetCLMBRClassifier, 
    get_patient_splits_by_idx
)
from scipy.sparse import issparse
import scipy
import lightgbm as lgb
import torch
from sklearn.model_selection import GridSearchCV, PredefinedSplit
from jaxtyping import Float
import femr
import femr.datasets
from femr.labelers import load_labeled_patients, LabeledPatients
from hf_ehr.utils import load_tokenizer_from_path, load_model_from_path
from hf_ehr.eval.ehrshot import CookbookModelWithClassificationHead
from ehrshot.labelers.omop import get_icu_visit_detail_codes, get_femr_codes
from ehrshot.labelers.ehrshot import (
    AnemiaInstantLabValueLabeler,
    HyperkalemiaInstantLabValueLabeler,
    HyponatremiaInstantLabValueLabeler,
    ThrombocytopeniaInstantLabValueLabeler,
    HypoglycemiaInstantLabValueLabeler,
    EssentialHypertensionCodeLabeler,
    AcuteMyocardialInfarctionCodeLabeler,
    PancreaticCancerCodeLabeler,
    CeliacDiseaseCodeLabeler,
    LupusCodeLabeler,
    HyperlipidemiaCodeLabeler,
)

MAX_SPECIAL_TOKEN_ID: int = 6 # ! Hardcoded based on tokenizer vocabulary
SPECIAL_TOKENS: Set[str] = set([ '[BOS]', '[EOS]', '[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]'])

def calc_metrics(y_test: Float[np.ndarray, 'N'], 
                 y_test_proba: Float[np.ndarray, 'N'], 
                 test_patient_ids: List[int]) -> Dict[str, Dict[str, float]]:
    """Calculates AUROC, AUPRC, and Brier scores for train, val, and test sets.
        NOTE: Expects `y_train_proba`, `y_val_proba`, and `y_test_proba` to be the probability of the positive class.
    """

    metric_dict = {
        'auroc': metrics.roc_auc_score,
        'brier': metrics.brier_score_loss,
        'auprc': metrics.average_precision_score,
    }
    
    # Calculate metrics
    scores = {}
    for metric, func in metric_dict.items():
        scores[metric] = {}
        test_score = func(y_test, y_test_proba)

        logger.info(f"Test {metric} score:  {test_score}")

        test_set = sorted(list(set(test_patient_ids)))

        score_list = []
        for i in range(1000): # 1k bootstrap replicates
            sample = sklearn.utils.resample(test_set, random_state=i)
            counts = collections.Counter(sample)
            weights = np.zeros_like(test_patient_ids)

            for i, p in enumerate(test_patient_ids):
                weights[i] = counts[p]

            score_val = func(y_test, y_test_proba, sample_weight=weights)
            score_list.append(score_val)

        # 95% CI
        lower, upper = np.percentile(score_list, [2.5, 97.5])

        # Std
        std = np.std(score_list, ddof=1)

        scores[metric]['score'] = test_score
        scores[metric]['std'] = std
        scores[metric]['lower'] = lower
        scores[metric]['mean'] = np.mean(score_list)
        scores[metric]['upper'] = upper
    return scores



################################################################################
# Zero-shot labelers
################################################################################
def _increment_days(n_days: int, token: str) -> int:
    """Increment `n_days` by the number of days specified in `token` under the CEHR-ATT tokenization scheme.
    If token isn't an ATT token, then do nothing."""
    if '[DAY' in token:
        n_days += int(token.replace('[DAY ', '').replace(']', ''))
    elif '[WEEK' in token:
        n_days += int(token.replace('[WEEK ', '').replace(']', '')) * 7
    elif '[MONTH' in token:
        n_days += int(token.replace('[MONTH ', '').replace(']', '')) * 30
    elif '[LONG TERM]' in token:
        n_days += 1080
    return n_days

def guo_icu_zeroshot_rollouts(timeline: List[str], ontology) -> bool:
    """Return TRUE if ICU visit occurs before "[VISIT END]" token. Otherwise, return FALSE."""
    icu_codes: Set[int] = get_icu_visit_detail_codes(ontology)
    visit_end_codes: Set[int] =  [ "[VISIT END]" ]
    for token in timeline:
        if token in icu_codes:
            return True
        if token in visit_end_codes:
            return False
        # Stop at first special token
        if token in SPECIAL_TOKENS:
            break
    return False

def _time_horizon_eval(timeline: List[str], positive_tokens: Set[str], time_horizon_days: int, is_less_than_or_equal: bool = True) -> bool:
    """Return TRUE if event occurs within `time_horizon` days. Otherwise, return FALSE."""
    n_days: int = 0
    for token in timeline:
        n_days = _increment_days(n_days, token)
        if token in positive_tokens:
            return n_days <= time_horizon_days if is_less_than_or_equal else n_days >= time_horizon_days
        # Stop at first special token
        if token in SPECIAL_TOKENS:
            break
    return False

def guo_los_zeroshot_rollouts(timeline: List[str], ontology) -> bool:
    """Return TRUE if LOS >= 7 days before "[VISIT END]" token. Otherwise, return FALSE."""
    positive_tokens: Set[int] =  [ "[VISIT END]" ]
    return _time_horizon_eval(timeline, positive_tokens, 7, is_less_than_or_equal=False)

def guo_readmission_zeroshot_rollouts(timeline: List[str], ontology) -> bool:
    """Return TRUE if "[VISIT START]" token occurs in <= 30 days. Otherwise, return FALSE."""
    positive_tokens: Set[int] =  [ "[VISIT START]" ]
    return _time_horizon_eval(timeline, positive_tokens, 30, is_less_than_or_equal=True)

def new_pancan_zeroshot_rollouts(timeline: List[str], ontology) -> bool:
    """Return TRUE if pancreatic cancer occurs in <= 365 days. Otherwise, return FALSE."""
    positive_tokens: Set[int] = list(get_femr_codes(ontology, [PancreaticCancerCodeLabeler.root_concept_code], is_ontology_expansion=True))
    return _time_horizon_eval(timeline, positive_tokens, 365, is_less_than_or_equal=True)

def new_celiac_zeroshot_rollouts(timeline: List[str], ontology) -> bool:
    """Return TRUE if celiac diagnosis occurs in <= 365 days. Otherwise, return FALSE."""
    positive_tokens: Set[int] = list(get_femr_codes(ontology, [CeliacDiseaseCodeLabeler.root_concept_code], is_ontology_expansion=True))
    return _time_horizon_eval(timeline, positive_tokens, 365, is_less_than_or_equal=True)

def new_lupus_zeroshot_rollouts(timeline: List[str], ontology) -> bool:
    """Return TRUE if lupus diagnosis occurs in <= 365 days. Otherwise, return FALSE."""
    positive_tokens: Set[int] = list(get_femr_codes(ontology, [LupusCodeLabeler.root_concept_code], is_ontology_expansion=True))
    return _time_horizon_eval(timeline, positive_tokens, 365, is_less_than_or_equal=True)

def new_acutemi_zeroshot_rollouts(timeline: List[str], ontology) -> bool:
    """Return TRUE if acute MI diagnosis occurs in <= 365 days. Otherwise, return FALSE."""
    positive_tokens: Set[int] = list(get_femr_codes(ontology, [AcuteMyocardialInfarctionCodeLabeler.root_concept_code], is_ontology_expansion=True))
    return _time_horizon_eval(timeline, positive_tokens, 365, is_less_than_or_equal=True)

def new_hypertension_zeroshot_rollouts(timeline: List[str], ontology) -> bool:
    """Return TRUE if hypertension diagnosis occurs in <= 365 days. Otherwise, return FALSE."""
    positive_tokens: Set[int] = list(get_femr_codes(ontology, [EssentialHypertensionCodeLabeler.root_concept_code], is_ontology_expansion=True))
    return _time_horizon_eval(timeline, positive_tokens, 365, is_less_than_or_equal=True)

def new_hyperlipidemia_zeroshot_rollouts(timeline: List[str], ontology) -> bool:
    """Return TRUE if hyperlipidemia diagnosis occurs in <= 365 days. Otherwise, return FALSE."""
    positive_tokens: Set[int] = list(get_femr_codes(ontology, [HyperlipidemiaCodeLabeler.root_concept_code], is_ontology_expansion=True))
    return _time_horizon_eval(timeline, positive_tokens, 365, is_less_than_or_equal=True)

def anemia_zeroshot_logits(logits: Float[torch.Tensor, 'N V'], ontology) -> bool:
    """Return TRUE if tokens corresponding to an abnormal anemia lab value have a greater cumulative probability than tokens for a normal readout. Otherwise, return FALSE."""
    # NOTE: This only works if abnormal/normal anemial labs are tokens in our vocabulary
    # Unfortunately, this is not the case for our current CLMBR vocab, so we can't do this zero-shot eval
    raise ValueError("Anemia zero-shot evaluation not implemented. Only works if abnormal/normal anemial labs are tokens in our vocabulary")

################################################################################
# END | Zero-shot labelers
################################################################################

################################################################################
# Zero-shot evaluations
################################################################################

def generate_rollouts(X_test_timelines: np.ndarray,
                        y_test: np.ndarray,
                        model: torch.nn.Module, 
                        pad_token_id: int,
                        model_name: str, 
                        batch_size: int,
                        model_max_length: int,
                        temperature: float,
                        max_tokens_to_generate: int,
                        seed: int,
                        device: str) -> Float[torch.Tensor, 'B max_tokens_to_generate']:
    """
    Generate `max_tokens_to_generate` tokens for each patient in `X_test_timelines`.
    
    Useful for New Diagnosis and Operational Outcome tasks where we have a time horizon to predict.
    
    Note that timelines are already left-padded with `pad_token_id` to the max length of the model,
    so we don't need to worry about that here.
    
    Returns a tensor of shape (N, max_tokens_to_generate)
    """
    torch.manual_seed(seed)
    model.eval()
    generations: List[torch.Tensor] = []
    with torch.no_grad():
        with torch.amp.autocast('cuda'):
            for batch_start in tqdm(range(0, X_test_timelines.shape[0], batch_size), desc=f'Rollout Inference: model={model_name[:15]}', total=X_test_timelines.shape[0] // batch_size):
                X_batch = X_test_timelines[batch_start:batch_start+batch_size]
                input_ids: Float[torch.Tensor, 'B model_max_length'] = torch.tensor(X_batch, device=device)
                
                # Generate `max_tokens_to_generate` tokens
                next_tokens: List[torch.Tensor] = []
                past_key_values: List[Tuple[torch.Tensor, torch.Tensor]] = None
                is_special_token_encountered = torch.zeros((X_batch.shape[0],1)).int() # track if any special token has been encountered so we can early stop
                for i in range(max_tokens_to_generate):
                    # Run model
                    if past_key_values is None or True:
                        # Full input for the first step
                        batch_input_ids = input_ids[:, -model_max_length:]
                        batch = {
                            "input_ids": batch_input_ids,  # Truncate to model max length
                            "attention_mask": (batch_input_ids != pad_token_id).int(),
                        }
                    else:
                        # Only last token for subsequent steps b/c using past key values cache
                        batch_input_ids = input_ids[:, -1:]
                        if 'gpt' in model_name:
                            batch = {
                                "input_ids": batch_input_ids,  # Last token only
                                "attention_mask": torch.ones_like(batch_input_ids, device=device),
                                "past_key_values": past_key_values,  # Pass cached states
                            }
                        elif 'llama' in model_name:
                            batch = {
                                "input_ids": batch_input_ids,  # Last token only
                                "attention_mask": torch.ones_like(batch_input_ids, device=device),
                                "past_key_values": past_key_values,  # Pass cached states
                            }
                        elif 'mamba' in model_name:
                            batch = {
                                "input_ids": batch_input_ids,  # Last token only
                                "attention_mask": torch.ones_like(batch_input_ids, device=device),
                                "use_cache": True,
                                'cache_params' : past_key_values,
                            }
                        elif 'hyena' in model_name:
                            batch = {
                                "input_ids": batch_input_ids,  # Last token only
                                "past_key_values": past_key_values,  # Pass cached states
                            }

                    outputs = model(**batch)
                    logits = outputs.logits
                    if 'gpt' in model_name or 'llama' in model_name:
                        past_key_values = outputs.past_key_values
                    elif 'hyena' in model_name:
                        past_key_values = outputs.past_key_values
                    elif 'mamba' in model_name:
                        past_key_values = outputs.cache_params

                    # Sample from logits with temperature
                    next_token_logits = logits[:, -1, :] / temperature  # Scale logits by temperature
                    next_token_probs = torch.nn.functional.softmax(next_token_logits, dim=-1)
                    next_token: Float[torch.Tensor, 'B 1'] = torch.multinomial(next_token_probs, num_samples=1)
                    input_ids: Float[torch.Tensor, 'B model_max_length+1'] = torch.cat([input_ids, next_token], dim=-1)
                    next_tokens.append(next_token)
                    
                    # Early stop if any special token is encountered (NOTE: This rarely happens, so just ignore it)
                    if False:
                        is_special_token_encountered = is_special_token_encountered | (next_token <= MAX_SPECIAL_TOKEN_ID).int().to('cpu')
                        # Early stop if every token in `next_token` is a `pad_token_id`. Fill in remaining tokens with `pad_token_id`.
                        if is_special_token_encountered.all():
                            next_tokens = next_tokens + [ torch.zeros((next_token.shape[0], max_tokens_to_generate - len(next_tokens)), device=device).fill_(pad_token_id) ]
                            break
                merged_tokens = torch.cat(next_tokens, dim=-1)
                assert merged_tokens.shape == (X_batch.shape[0], max_tokens_to_generate), f"Merged tokens have incorrect shape: {merged_tokens.shape}"
                generations.append(merged_tokens)
    generations: Float[torch.Tensor, 'N max_tokens_to_generate'] = torch.cat(generations, dim=0)
    assert generations.shape == (X_test_timelines.shape[0], max_tokens_to_generate), f"Generations have incorrect shape: {generations.shape}"
    return generations


def eval_rollouts(generations: List[Float[torch.Tensor, 'N max_tokens_to_generate']], is_positive_func: Callable, idx_2_token: Dict[int, str], ontology) -> np.ndarray:
    """Evaluate if a rollout contains a positive event.
    
    `generations` is a list of 20 tensors, each of shape (N, max_tokens_to_generate)
    """
    labels: List[List[bool]] = [ 
        [ is_positive_func([ idx_2_token[tok.item()] for tok in timeline ], ontology) for timeline in gens ] 
        for gens in generations
    ]
    labels: Float[np.ndarray, 'N generations'] = np.array(labels).T # each row is a data point, each column a generation
    assert labels.shape == (len(generations[0]), len(generations)), f"Labels have incorrect shape: {labels.shape} != {(len(generations), 20)}"
    y_test_proba: Float[np.ndarray, 'N'] = np.mean(labels, axis=1) # Take mean across trials as "probability" of event occuring
    return y_test_proba

def run_zeroshot_rollout_evals(X_test: np.ndarray, 
                                y_test: np.ndarray,
                                X_test_timelines: np.ndarray,
                                model_name: str, 
                                sub_task: str,
                                path_to_ckpt: str,
                                batch_size: int = 4,
                                temperature: float = 0.7,
                                max_tokens_to_generate: int = 128,
                                start_generation_idx: int = 0,
                                end_generation_idx: int = 20,
                                ontology = None,
                                test_patient_ids: List[int] = None,
                                device: str = 'cuda',
                                path_to_generations_dir: str = None,
                                is_force_refresh: bool = False) -> Tuple[Dict[str, float], Dict[str, Dict[str, float]]]:
    """Run ETHOS-style zero-shot evaluation on timelines. Implementation depends on the sub_task."""
    logger.info(f"Test prevalence:  {np.mean(y_test)}")
    logger.info(f"Test pids:  {len(test_patient_ids)} | {len(y_test)} | {len(set(test_patient_ids))}")

    # Load tokenizer
    tokenizer = load_tokenizer_from_path(path_to_ckpt)
    pad_token_id: int = tokenizer.pad_token_id
    embed_strat: str = [ x for x in model_name.split("_") if x.split(":")[0] == 'embed' ][0].split(":")[1] # "mean" or "last"

    # Load original model -- weights won't change
    model = load_model_from_path(path_to_ckpt)
    model.get_params = lambda : {}
    model.model.to(device)
 
    # # TODO - debugging
    # X_test_timelines = X_test_timelines[:10]
    # y_test = y_test[:10]
    # test_patient_ids = test_patient_ids[:10]
    
    # Eval best model on train/val/test sets
    logger.info(f"Start | Generating zero-shot sequences=`{model_name}`")
    generations: List[Float[torch.Tensor, 'N max_tokens_to_generate']] = []
    for i in range(start_generation_idx, end_generation_idx):
        path_to_cached_rollouts: str = os.path.join(path_to_generations_dir, f'rollouts_{model_name}_{sub_task}_{i}.pt')
        if os.path.exists(path_to_cached_rollouts) and not is_force_refresh:
            gens = torch.load(path_to_cached_rollouts, map_location='cpu')
            print(f"Loaded cached rollouts from {path_to_cached_rollouts}")
            assert gens.shape == (X_test_timelines.shape[0], max_tokens_to_generate), f"Cached rollouts have incorrect shape: {gens.shape}"
        else:
            gens: Float[torch.Tensor, 'N max_tokens_to_generate'] = generate_rollouts(
                X_test_timelines, 
                y_test, 
                model.model, 
                pad_token_id, 
                model_name, 
                batch_size,
                model.config.data.dataloader.max_length,
                temperature,
                max_tokens_to_generate,
                seed=i,
                device=device
            ).to('cpu')
            torch.save(gens, path_to_cached_rollouts)
        generations.append(gens)
    logger.info(f"Finish | Generating zero-shot sequences=`{model_name}`")
    model.to('cpu')

    # Run ETHOS-style evaluation
    if sub_task == 'guo_icu':
        # TRUE if ICU transfer before "[VISIT END]" token
        y_test_proba: Float[np.ndarray, 'N'] = eval_rollouts(generations, guo_icu_zeroshot_rollouts, tokenizer.idx_2_token, ontology)
    elif sub_task == 'guo_los':
        # TRUE if >= 7 days before "[VISIT END]" token
        y_test_proba = eval_rollouts(generations, guo_los_zeroshot_rollouts, tokenizer.idx_2_token, ontology)
    elif sub_task == 'guo_readmission':
        # TRUE if "[VISIT START]" token occurs within 30 days
        y_test_proba = eval_rollouts(generations, guo_readmission_zeroshot_rollouts, tokenizer.idx_2_token, ontology)
    elif sub_task == 'new_pancan':
        # TRUE if pancreatic cancer occurs within 365 days
        y_test_proba = eval_rollouts(generations, new_pancan_zeroshot_rollouts, tokenizer.idx_2_token, ontology)
    elif sub_task == 'new_celiac':
        # TRUE if celiac diagnosis occurs within 365 days
        y_test_proba = eval_rollouts(generations, new_celiac_zeroshot_rollouts, tokenizer.idx_2_token, ontology) 
    elif sub_task == 'new_lupus':
        # TRUE if lupus diagnosis occurs within 365 days
        y_test_proba = eval_rollouts(generations, new_lupus_zeroshot_rollouts, tokenizer.idx_2_token, ontology)  
    elif sub_task == 'new_acutemi':
        # TRUE if acute MI diagnosis occurs within 365 days 
        y_test_proba = eval_rollouts(generations, new_acutemi_zeroshot_rollouts, tokenizer.idx_2_token, ontology)
    elif sub_task == 'new_hypertension':
        # TRUE if hypertension diagnosis occurs within 365 days
        y_test_proba = eval_rollouts(generations, new_hypertension_zeroshot_rollouts, tokenizer.idx_2_token, ontology)
    elif sub_task == 'new_hyperlipidemia':
        # TRUE if hyperlipidemia diagnosis
        y_test_proba = eval_rollouts(generations, new_hyperlipidemia_zeroshot_rollouts, tokenizer.idx_2_token, ontology)
    else:
        raise ValueError(f"Unknown sub_task: {sub_task} for zero-shot rollout evaluation")
    
    # Calculate AUROC, AUPRC, and Brier scores
    best_scores: Dict[str, float] = calc_metrics(y_test, y_test_proba, test_patient_ids)
    best_metrics: Dict[str, np.ndarray] = { 'test' : y_test_proba }

    # Return scores
    return best_scores, best_metrics


################################################################################
# END | Zero-shot evaluation functions
################################################################################

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run EHRSHOT evaluation benchmark on a specific task.")
    parser.add_argument("--path_to_database", required=True, type=str, help="Path to FEMR patient database")
    parser.add_argument("--path_to_labels_dir", required=True, type=str, help="Path to directory containing saved labels")
    parser.add_argument("--path_to_features_dir", required=True, type=str, help="Path to directory containing saved features")
    parser.add_argument("--path_to_output_dir", required=True, type=str, help="Path to directory where results will be saved")
    parser.add_argument("--path_to_split_csv", required=True, type=str, help="Path to CSV of splits")
    parser.add_argument("--path_to_tokenized_timelines_dir", required=False, type=str, default=None, help="Path to directory containing tokenized timelines (if applicable)")
    parser.add_argument("--shot_strat", type=str, choices=SHOT_STRATS.keys(), help="What type of X-shot evaluation we are interested in.", required=True )
    parser.add_argument("--labeling_function", required=True, type=str, help="Labeling function for which we will create k-shot samples.", choices=LABELING_FUNCTION_2_PAPER_NAME.keys(), )
    parser.add_argument("--is_force_refresh", action='store_true', default=False, help="If set, then overwrite all outputs")
    parser.add_argument("--models", required=True, help="Comma separated list. If specified, then only consider models in this list, e.g. `clmbr,count`")
    parser.add_argument("--heads", default=None, help="Comma separated list. If specified, then only consider heads in this list, e.g. `finetune_layers=1,finetune_layers=2`")
    parser.add_argument("--batch_size", type=int, default=4, help="Batch size for finetuning. NOTE: This must be a small value (e.g. 4) for torch.compile() to correctly work")
    parser.add_argument("--max_tokens_to_generate", type=int, default=128, help="Maximum number of tokens to generate")
    parser.add_argument("--temperature", type=float, default=0.7, help="Temperature for rollout generation")
    parser.add_argument("--start_generation_idx", type=int, default=0, help="Start index for generation rollouts")
    parser.add_argument("--end_generation_idx", type=int, default=20, help="End index for generation rollouts (exclusive)")
    parser.add_argument("--device", type=str, default='cuda', help="Device to run the model on")
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    PATH_TO_PATIENT_DATABASE = args.path_to_database
    LABELING_FUNCTION: str = args.labeling_function
    VALID_MODELS: List[str] = args.models.split(',')
    SHOT_STRAT: str = args.shot_strat
    BATCH_SIZE: int = args.batch_size
    DEVICE: str = args.device
    max_tokens_to_generate: int = args.max_tokens_to_generate
    temperature: float = args.temperature
    start_generation_idx: int = args.start_generation_idx
    end_generation_idx: int = args.end_generation_idx
    PATH_TO_TOKENIZED_TIMELINES_DIR: Optional[str] = args.path_to_tokenized_timelines_dir
    IS_FORCE_REFRESH: bool = args.is_force_refresh
    PATH_TO_FEATURES_DIR: str = args.path_to_features_dir
    PATH_TO_LABELS_DIR: str = args.path_to_labels_dir
    PATH_TO_SPLIT_CSV: str = args.path_to_split_csv
    PATH_TO_LABELED_PATIENTS: str = os.path.join(PATH_TO_LABELS_DIR, LABELING_FUNCTION, 'labeled_patients.csv')
    PATH_TO_OUTPUT_DIR: str = args.path_to_output_dir
    PATH_TO_OUTPUT_FILE: str = os.path.join(PATH_TO_OUTPUT_DIR, LABELING_FUNCTION, f'{SHOT_STRAT}_results.csv')
    PATH_TO_GENERATIONS_DIR: Optional[str] = os.path.abspath(os.path.join(PATH_TO_OUTPUT_DIR, '../zeroshot_ehrshot/'))
    os.makedirs(os.path.dirname(PATH_TO_OUTPUT_FILE), exist_ok=True)
    
    # Load ontology
    database = PatientDatabase(PATH_TO_PATIENT_DATABASE)
    ontology = database.get_ontology()

    # Determine which models to load
    logger.critical(f"Only running models: {VALID_MODELS}")

    # Load all labeled patients
    labeled_patients: LabeledPatients = load_labeled_patients(PATH_TO_LABELED_PATIENTS)
    logger.info(f"Loading task {LABELING_FUNCTION} with {len(labeled_patients)} labeled patients.")
    
    # No real head b/c zero-shot
    head: str = 'zero_shot'
    
    # For each base model we are evaluating...
    for model in VALID_MODELS:
        # Get path to saved model
        path_to_model_dir: str = os.path.abspath(os.path.join(PATH_TO_FEATURES_DIR, '../models', model))
        if model == 'clmbr':
            path_to_ckpt: str = os.path.join(path_to_model_dir, 'clmbr_model/best')
        else:
            path_to_ckpt: str = os.path.join(path_to_model_dir, [ x for x in os.listdir(path_to_model_dir) if x.endswith('.ckpt') ][0])
    
        assert PATH_TO_TOKENIZED_TIMELINES_DIR is not None, "Must specify `--path_to_tokenized_timelines_dir` for finetuning experiments"

        # Load labels/features for this task + model_head
        patient_ids, label_values, label_times, feature_matrixes = get_labels_and_features(labeled_patients, 
                                                                                            PATH_TO_FEATURES_DIR, 
                                                                                            PATH_TO_TOKENIZED_TIMELINES_DIR,
                                                                                            models_to_keep=[ model ])
        __, __, test_pids_idx = get_patient_splits_by_idx(PATH_TO_SPLIT_CSV, patient_ids)

        # Preprocess certain non-binary labels
        if LABELING_FUNCTION == "chexpert":
            label_values = process_chexpert_labels(label_values)
            sub_tasks: List[str] = CHEXPERT_LABELS
        elif LABELING_FUNCTION.startswith('lab_'):
        # Lab value is multi-class, convert to binary
            label_values = convert_multiclass_to_binary_labels(label_values, threshold=1)
            sub_tasks: List[str] = [LABELING_FUNCTION]
        else:
            # Binary classification
            sub_tasks: List[str] = [LABELING_FUNCTION]
            
        assert model in feature_matrixes, f"Feature matrix not found for `{model}`. Are you sure you have generated features for this model? If not, you'll need to rerun `generate_features.py` or `generate_clmbr_representations.py`."
        
        # Unpack each individual featurization we want to test
        assert torch.cuda.is_available(), "CUDA must be available to run PyTorch models."
        X_test_timelines: np.ndarray = feature_matrixes[model]['timelines'][test_pids_idx]
        X_test: np.ndarray = feature_matrixes[model]['frozen'][test_pids_idx]
        
        # Test labels
        y_test: np.ndarray = label_values[test_pids_idx]
        test_patient_ids = patient_ids[test_pids_idx]
        
        # Always just do k=-1
        k = -1
        replicate = 0
        
        # For each subtask in this task... 
        # NOTE: The "subtask" is just the same thing as LABELING_FUNCTION for all binary tasks.
        # But for Chexpert, there are multiple subtasks, which of each represents a binary subtask
        for sub_task_idx, sub_task in enumerate(sub_tasks):
            ############################
            # ! Do loading of previous results here so that we reload any results that
            # ! were generated in a concurrently running SLURM job
            # If results already exist, then append new results to existing file
            df_existing: Optional[pd.DataFrame] = None
            if os.path.exists(PATH_TO_OUTPUT_FILE):
                logger.warning(f"Results already exist @ `{PATH_TO_OUTPUT_FILE}`.")
                df_existing = pd.read_csv(PATH_TO_OUTPUT_FILE)
            ############################
            
            # Check if results already exist for this model/head/shot_strat in `results.csv`
            if df_existing is not None:
                existing_rows: pd.DataFrame = df_existing[
                    (df_existing['labeling_function'] == LABELING_FUNCTION) 
                    & (df_existing['sub_task'] == sub_task) 
                    & (df_existing['model'] == model) 
                    & (df_existing['head'] == head)
                    & (df_existing['k'] == k)
                    & (df_existing['replicate'] == replicate)
                ]
                if existing_rows.shape[0] > 0:
                    # Overwrite
                    if IS_FORCE_REFRESH:
                        logger.warning(f"Results ALREADY exist for {model}/{head}/{LABELING_FUNCTION}/{sub_task}/k={k}/r={replicate} in `results.csv`.\nOVERWRITING these rows because `is_force_refresh` is TRUE.")
                    else:
                        logger.warning(f"Results ALREADY exist for {model}/{head}/{LABELING_FUNCTION}/{sub_task}/k={k}/r={replicate} in `results.csv`.\nSKIPPING this combination because `is_force_refresh` is FALSE.")
                        continue
                else:
                    # Append
                    logger.warning(f"Results DO NOT exist for {model}/{head}/{LABELING_FUNCTION}/{sub_task}/k={k}/r={replicate} in `results.csv`. Appending to this CSV.")
    
            logger.success(f"Model: {model} | Head: {head} | Task: {sub_task} | k: {k} | replicate: {replicate}")
            
            # Test labels
            y_test_k: np.ndarray = np.array(y_test)

            # CheXpert adjustment
            if LABELING_FUNCTION == 'chexpert':
                y_test_k = y_test[:, sub_task_idx]

            # Run eval
            assert os.path.exists(path_to_ckpt), f"Path to .ckpt for model={model} does not exist: `{path_to_ckpt}`"
            logger.info(f"Loaded model `{model}` from .ckpt at: `{path_to_ckpt}`")
            
            # Evaluate generations following ETHOS method
            if sub_task.startswith('guo') or sub_task.startswith('new_'):
                # Tasks that require full rollout generation to evaluate
                scores, metrics = run_zeroshot_rollout_evals(
                    X_test, 
                    y_test_k, 
                    X_test_timelines,
                    model_name=model, 
                    sub_task=sub_task,
                    path_to_ckpt=path_to_ckpt, 
                    batch_size=BATCH_SIZE,
                    temperature=temperature,
                    max_tokens_to_generate=max_tokens_to_generate,
                    start_generation_idx=start_generation_idx,
                    end_generation_idx=end_generation_idx,
                    ontology=ontology,
                    test_patient_ids=test_patient_ids,
                    path_to_generations_dir=PATH_TO_GENERATIONS_DIR,
                    device=DEVICE,
                    is_force_refresh=IS_FORCE_REFRESH,
                )
            elif sub_task.startswith('lab_'):
                raise NotImplementedError("Zero-shot next token logits evaluation is not yet implemented")
                # Tasks that require next token logits to evaluate
                scores, metrics = run_zeroshot_next_token_evals(
                    X_test, 
                    y_test_k, 
                    X_test_timelines,
                    model_name=model, 
                    sub_task=sub_task,
                    path_to_ckpt=path_to_ckpt, 
                    batch_size=BATCH_SIZE,
                    ontology=ontology,
                    test_patient_ids=test_patient_ids,
                    is_force_refresh=IS_FORCE_REFRESH,
                )
            else:
                raise ValueError(f"Unknown sub_task: {sub_task} for zero-shot evaluation")

            # Save results
            results: List[Dict[str, Any]] = []
            for score_name, score_value in scores.items():
                results.append({
                    'labeling_function' : LABELING_FUNCTION,
                    'sub_task' : sub_task,
                    'model' : model,
                    'head' : head,
                    'replicate' : replicate,
                    'k' : k,
                    'score' : score_name,
                    'value' : score_value['score'],
                    'std' : score_value['std'],
                    'lower' : score_value['lower'],
                    'mean' : score_value['mean'],
                    'upper' : score_value['upper'],
                    'y_test_proba' : metrics['test'].tolist(),
                    'model_hparams' : { 'start_idx' : start_generation_idx, 'end_idx' : end_generation_idx },
                })

            ############################
            # Reload to ensure we have most updated version of file
            if os.path.exists(PATH_TO_OUTPUT_FILE):
                df_existing = pd.read_csv(PATH_TO_OUTPUT_FILE)
                results += df_existing.to_dict(orient='records')
            ############################

            # Save results to CSV after each (model, head, sub_task, k, replicate) is calculated
            logger.critical(f"Saving results for {model} + {head} + k={k} + replicate={replicate} to: {PATH_TO_OUTPUT_FILE}")
            df: pd.DataFrame = pd.DataFrame(results)
            logger.critical(f"Added {df.shape[0] - (df_existing.shape[0] if df_existing is not None else 0)} rows")
            df.to_csv(PATH_TO_OUTPUT_FILE, index=False)
    logger.success(f"Done with {model} for task {LABELING_FUNCTION}!")

