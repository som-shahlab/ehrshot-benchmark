{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test LLM2Vec\n",
    "* Simple Prototype for LLM endcoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bf41916b154aaca85300cc005b60da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66050e50c7fb4477a8c97fff66acf32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from llm2vec import LLM2Vec\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = LLM2Vec.from_pretrained(\n",
    "    \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\",\n",
    "    peft_model_name_or_path=\"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised\",\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545194f505d14bbc88a7fa081c870f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaBiModel' object has no attribute 'rotary_emb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_texts\u001b[39m(texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(model\u001b[38;5;241m.\u001b[39mencode(texts, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m))\n\u001b[0;32m---> 20\u001b[0m merged_encodings \u001b[38;5;241m=\u001b[39m \u001b[43mencode_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmerged_serializations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m, in \u001b[0;36mencode_texts\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_texts\u001b[39m(texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/sc-projects/sc-proj-dh-ag-eils-ml/shared_envs/EHRSHOT_ENV/lib/python3.10/site-packages/llm2vec/llm2vec.py:361\u001b[0m, in \u001b[0;36mLLM2Vec.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, convert_to_numpy, convert_to_tensor, device)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28mlen\u001b[39m(sentences),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    356\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar,\n\u001b[1;32m    357\u001b[0m     ):\n\u001b[1;32m    358\u001b[0m         sentences_batch \u001b[38;5;241m=\u001b[39m sentences_sorted[\n\u001b[1;32m    359\u001b[0m             start_index : start_index \u001b[38;5;241m+\u001b[39m batch_size\n\u001b[1;32m    360\u001b[0m         ]\n\u001b[0;32m--> 361\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m            \u001b[49m\u001b[43msentences_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_to_numpy\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m         all_embeddings\u001b[38;5;241m.\u001b[39mappend(embeddings)\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/sc-projects/sc-proj-dh-ag-eils-ml/shared_envs/EHRSHOT_ENV/lib/python3.10/site-packages/llm2vec/llm2vec.py:447\u001b[0m, in \u001b[0;36mLLM2Vec._encode\u001b[0;34m(self, sentences_batch, device, convert_to_numpy, multiprocessing)\u001b[0m\n\u001b[1;32m    444\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 447\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    449\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m/sc-projects/sc-proj-dh-ag-eils-ml/shared_envs/EHRSHOT_ENV/lib/python3.10/site-packages/llm2vec/llm2vec.py:227\u001b[0m, in \u001b[0;36mLLM2Vec.forward\u001b[0;34m(self, sentence_feature)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sentence_feature:\n\u001b[1;32m    226\u001b[0m     embed_mask \u001b[38;5;241m=\u001b[39m sentence_feature\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 227\u001b[0m reps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msentence_feature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m sentence_feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m embed_mask\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_pooling(sentence_feature, reps\u001b[38;5;241m.\u001b[39mlast_hidden_state)\n",
      "File \u001b[0;32m/sc-projects/sc-proj-dh-ag-eils-ml/shared_envs/EHRSHOT_ENV/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sc-projects/sc-proj-dh-ag-eils-ml/shared_envs/EHRSHOT_ENV/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/sc-projects/sc-proj-dh-ag-eils-ml/shared_envs/EHRSHOT_ENV/lib/python3.10/site-packages/peft/peft_model.py:762\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    761\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sc-projects/sc-proj-dh-ag-eils-ml/shared_envs/EHRSHOT_ENV/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sc-projects/sc-proj-dh-ag-eils-ml/shared_envs/EHRSHOT_ENV/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/sc-projects/sc-proj-dh-ag-eils-ml/shared_envs/EHRSHOT_ENV/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:920\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    917\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[1;32m    919\u001b[0m \u001b[38;5;66;03m# create position embeddings to be shared across the decoder layers\u001b[39;00m\n\u001b[0;32m--> 920\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_emb\u001b[49m(hidden_states, position_ids)\n\u001b[1;32m    922\u001b[0m \u001b[38;5;66;03m# decoder layers\u001b[39;00m\n\u001b[1;32m    923\u001b[0m all_hidden_states \u001b[38;5;241m=\u001b[39m () \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/sc-projects/sc-proj-dh-ag-eils-ml/shared_envs/EHRSHOT_ENV/lib/python3.10/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaBiModel' object has no attribute 'rotary_emb'"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Deque, Dict, Iterable, Iterator, List, Optional, Set, Tuple\n",
    "import numpy as np\n",
    "\n",
    "serial1: dict[int, str] = {}\n",
    "serial1[123] = \"hello world\"\n",
    "serial1[456] = \"goodbye world\"\n",
    "\n",
    "serial2: dict[int, str] = {}\n",
    "serial2[223] = \"hello world\"\n",
    "serial2[789] = \"goodbye world\"\n",
    "\n",
    "serializations = [serial1, serial2]\n",
    "\n",
    "merged_serializations: dict[int, str] = {}\n",
    "for s in serializations:\n",
    "    merged_serializations.update(s)\n",
    "\n",
    "def encode_texts(texts: List[str]) -> List[np.ndarray]:\n",
    "    return np.array(model.encode(texts, batch_size=32))\n",
    "merged_encodings = encode_texts(list(merged_serializations.values())) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/sc-projects/sc-proj-dh-ag-eils-ml/shared_envs/EHRSHOT_ENV/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 4/4 [00:00<00:00,  5.40it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.51it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  3.45it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of numpy.int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m      5\u001b[0m patients \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe patient is a 23 year-old male of caucasian race.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe patient is a 32 year-old male of caucasian race.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe patient is a 83 year-old female of black race.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m ]   \n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m LLM2Vec\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMcGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     peft_model_name_or_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMcGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m patients_encoded \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/sc-projects/sc-proj-dh-ag-eils-ml/shared_envs/EHRSHOT_ENV/lib/python3.10/site-packages/llm2vec/llm2vec.py:398\u001b[0m, in \u001b[0;36mLLM2Vec.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, convert_to_numpy, convert_to_tensor, device)\u001b[0m\n\u001b[1;32m    395\u001b[0m         progress_bar\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    397\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_embeddings, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 398\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mall_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength_sorted_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    399\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m all_embeddings\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of numpy.int64"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llm2vec import LLM2Vec\n",
    "import numpy as np\n",
    "\n",
    "patients = [\n",
    "    'The patient is a 23 year-old male of caucasian race.',\n",
    "    'The patient is a 32 year-old male of caucasian race.',\n",
    "    'The patient is a 83 year-old female of black race.',\n",
    "]   \n",
    "\n",
    "\n",
    "model = LLM2Vec.from_pretrained(\n",
    "    \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\",\n",
    "    peft_model_name_or_path=\"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised\",\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "patients_encoded = np.array(model.encode(patients, batch_size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Classify the following description of a patient as being at risk for death or not: \"\n",
    "\n",
    "def append_instruction(instruction, sentences):\n",
    "    new_sentences = []\n",
    "    for s in sentences:\n",
    "        new_sentences.append([instruction, s, 0])\n",
    "    return new_sentences\n",
    "\n",
    "patients = [\n",
    "    'The patient is a 23 year-old male of caucasian race.',\n",
    "    'The patient is a 32 year-old male of caucasian race.',\n",
    "    'The patient is a 83 year-old female of black race.',\n",
    "]\n",
    "patients = 1300 * patients\n",
    "\n",
    "patients_instructions = append_instruction(instruction, patients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 122/122 [00:14<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.8653, 0.4906,  ..., 1.0000, 0.8653, 0.4906],\n",
      "        [0.8653, 1.0000, 0.4857,  ..., 0.8653, 1.0000, 0.4857],\n",
      "        [0.4906, 0.4857, 1.0000,  ..., 0.4906, 0.4857, 1.0000],\n",
      "        ...,\n",
      "        [1.0000, 0.8653, 0.4906,  ..., 1.0000, 0.8653, 0.4906],\n",
      "        [0.8653, 1.0000, 0.4857,  ..., 0.8653, 1.0000, 0.4857],\n",
      "        [0.4906, 0.4857, 1.0000,  ..., 0.4906, 0.4857, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "batch_size = 32\n",
    "\n",
    "# patients_encoded = np.asarray(model.encode(patients, batch_size=batch_size))\n",
    "patients_encoded = np.array(model.encode(patients, batch_size=batch_size))\n",
    "\n",
    "# Compute cosine similarity\n",
    "p_encoded_norm = torch.nn.functional.normalize(patients_encoded, p=2, dim=1)\n",
    "cos_sim = torch.mm(p_encoded_norm, p_encoded_norm.transpose(0, 1))\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6469, 0.1619],\n",
      "        [0.0789, 0.5837]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntensor([[0.6470, 0.1619],\\n        [0.0786, 0.5844]])\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding queries using instructions\n",
    "instruction = (\n",
    "    \"Given a web search query, retrieve relevant passages that answer the query:\"\n",
    ")\n",
    "queries = [\n",
    "    [instruction, \"how much protein should a female eat\"],\n",
    "    [instruction, \"summit define\"],\n",
    "]\n",
    "q_reps = l2v.encode(queries)\n",
    "\n",
    "# Encoding documents. Instruction are not required for documents\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "    \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\",\n",
    "]\n",
    "d_reps = l2v.encode(documents)\n",
    "\n",
    "# Compute cosine similarity\n",
    "q_reps_norm = torch.nn.functional.normalize(q_reps, p=2, dim=1)\n",
    "d_reps_norm = torch.nn.functional.normalize(d_reps, p=2, dim=1)\n",
    "cos_sim = torch.mm(q_reps_norm, d_reps_norm.transpose(0, 1))\n",
    "\n",
    "print(cos_sim)\n",
    "\"\"\"\n",
    "tensor([[0.6470, 0.1619],\n",
    "        [0.0786, 0.5844]])\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EHRSHOT_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
